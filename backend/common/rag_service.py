import os
import json
import logging
import boto3
from supabase import create_client
from typing import List, Dict, Any, Optional

logger = logging.getLogger(__name__)

class RAGService:
    def __init__(self):
        # Initialize Supabase
        self.supabase_url = os.getenv("SUPABASE_URL")
        self.supabase_key = os.getenv("SUPABASE_SERVICE_KEY")
        if not self.supabase_url or not self.supabase_key:
            raise ValueError("SUPABASE_URL and SUPABASE_SERVICE_KEY must be set")
        self.supabase = create_client(self.supabase_url, self.supabase_key)

        # Initialize Bedrock
        self.aws_region = os.getenv("AWS_REGION", "ap-northeast-2")
        self.bedrock = boto3.client(service_name='bedrock-runtime', region_name=self.aws_region)
        
        # Models
        self.embedding_model_id = "amazon.titan-embed-text-v2:0"
        self.llm_model_id = "anthropic.claude-3-haiku-20240307-v1:0" 

    def generate_embedding(self, text: str) -> Optional[List[float]]:
        """Generate embedding using Amazon Titan Text Embeddings v2"""
        if not text:
            return None
            
        try:
            body = json.dumps({
                "inputText": text,
                "dimensions": 1024,
                "normalize": True
            })
            
            response = self.bedrock.invoke_model(
                body=body,
                modelId=self.embedding_model_id,
                accept="application/json",
                contentType="application/json"
            )
            
            response_body = json.loads(response.get('body').read())
            return response_body.get('embedding')
        except Exception as e:
            logger.error(f"Failed to generate embedding: {e}")
            return None

    def search_benefits(self, query_text: str, user_profile: Dict[str, Any], limit: int = 5) -> List[Dict[str, Any]]:
        """
        Execute Hybrid Search (Semantic + Keyword + Metadata Filters)
        user_profile should contain: 'ctpv_nm', 'sgg_nm', 'interest_ages' (list)
        """
        # 1. Generate Query Embedding
        query_embedding = self.generate_embedding(query_text)
        if not query_embedding:
            logger.warning("Could not generate embedding for query.")
            return []

        # 2. Call Supabase RPC
        # RPC signature: search_benefits_hybrid(query_embedding, user_ctpv_nm, user_sgg_nm, user_interest_ages, limit_count)
        try:
            params = {
                "query_embedding": query_embedding,
                "user_ctpv_nm": user_profile.get("ctpv_nm", ""),
                "user_sgg_nm": user_profile.get("sgg_nm", ""),
                "user_interest_ages": user_profile.get("interest_ages", []),
                "limit_count": limit
            }
            
            response = self.supabase.rpc("search_benefits_hybrid", params).execute()
            return response.data
        except Exception as e:
            logger.error(f"Search failed: {e}")
            return []

    def generate_answer(self, query_text: str, context_docs: List[Dict[str, Any]], stream: bool = False) -> Any:
        """
        Generate answer using Claude 3 Haiku
        If stream=True, returns a generator. Otherwise returns full string.
        """
        
        # Build Context String
        context_str = ""
        for i, doc in enumerate(context_docs, 1):
            context_str += f"[{i}] {doc['title']}\n"
            context_str += f"   - ë‚´ìš©: {doc['content']}\n"
            context_str += f"   - ë§í¬: {doc['original_url']}\n\n"
            
        if not context_str:
            context_str = "ê²€ìƒ‰ëœ ê´€ë ¨ ë³µì§€ í˜œíƒì´ ì—†ìŠµë‹ˆë‹¤."

        # System Prompt
        system_prompt = """ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ì˜ ë³µì§€ í˜œíƒì„ ì¹œì ˆí•˜ê²Œ ì•Œë ¤ì£¼ëŠ” 'ë˜‘ìˆœì´'ì…ë‹ˆë‹¤.
ì£¼ì–´ì§„ <context> ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.
- ì‚¬ìš©ìê°€ ê±°ì£¼í•˜ëŠ” ì§€ì—­ê³¼ ì—°ë ¹ëŒ€ì— ë§ëŠ” í˜œíƒ ìœ„ì£¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”.
- ê° í˜œíƒì˜ 'ì§€ì› ëŒ€ìƒ', 'ì§€ì› ë‚´ìš©', 'ì‹ ì²­ ë°©ë²•'ì„ ëª…í™•íˆ êµ¬ë¶„í•´ ì„¤ëª…í•˜ì„¸ìš”.
- ë‹µë³€ ëì—ëŠ” ë°˜ë“œì‹œ ì¶œì²˜(ë§í¬)ë¥¼ í¬í•¨í•˜ì„¸ìš”.
- <context>ì— ì—†ëŠ” ë‚´ìš©ì€ ì§€ì–´ë‚´ì§€ ë§ê³ , ì •ë³´ê°€ ë¶€ì¡±í•˜ë©´ ì†”ì§íˆ ë§í•´ì£¼ì„¸ìš”.
- í•­ìƒ ì¹œì ˆí•˜ê³  ì •ì¤‘í•œ ì–´ì¡°('~í•´ìš”'ì²´)ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”."""

        # Messages
        messages = [
            {
                "role": "user",
                "content": f"""<context>
{context_str}
</context>

ì§ˆë¬¸: {query_text}"""
            }
        ]

        body = json.dumps({
            "anthropic_version": "bedrock-2023-05-31",
            "max_tokens": 1000,
            "system": system_prompt,
            "messages": messages,
            "temperature": 0.5
        })

        try:
            if stream:
                response = self.bedrock.invoke_model_with_response_stream(
                    body=body,
                    modelId=self.llm_model_id
                )
                return response.get('body')
            else:
                response = self.bedrock.invoke_model(
                    body=body,
                    modelId=self.llm_model_id
                )
                response_body = json.loads(response.get('body').read())
                return response_body['content'][0]['text']
                
        except Exception as e:
            logger.error(f"LLM Generation failed: {e}")
            return "ì£„ì†¡í•´ìš”, ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ë„ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆì–´ìš”. ğŸ˜¢"

    def get_response(self, query_text: str, user_profile: Dict[str, Any], stream: bool = False):
        """
        Main RAG Workflow: Search -> Generate
        Returns (context, answer)
        """
        # 1. Search
        context_docs = self.search_benefits(query_text, user_profile)
        
        # 2. Generate
        answer = self.generate_answer(query_text, context_docs, stream=stream)
        
        return context_docs, answer
